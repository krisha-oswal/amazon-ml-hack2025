"""
EMERGENCY FIX - With TF-IDF Features
Complete working solution with text features
"""

import pandas as pd
import numpy as np
import re
import lightgbm as lgb
from sklearn.model_selection import KFold
from sklearn.feature_extraction.text import TfidfVectorizer
import warnings
warnings.filterwarnings('ignore')

def smape(y_true, y_pred):
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    diff = np.abs(y_true - y_pred) / denominator
    diff[denominator == 0] = 0.0
    return 100 * np.mean(diff)

print("="*70)
print("EMERGENCY FIX - WITH TF-IDF FEATURES")
print("="*70)

# Load data
print("\n1. Loading data...")
train = pd.read_csv('dataset/train.csv')
test = pd.read_csv('dataset/test.csv')
print(f"Train: {len(train):,}, Test: {len(test):,}")

# Show sample data
print("\n2. Sample data check:")
print(train[['sample_id', 'catalog_content', 'price']].head(3))

# Extract IPQ - most important feature
print("\n3. Extracting IPQ (Item Pack Quantity)...")

def extract_ipq(text):
    if pd.isna(text):
        return 1
    
    text = str(text).lower()
    
    # Check explicit patterns first
    if 'pack of 12' in text or '12 pack' in text or '12-pack' in text:
        return 12
    if 'pack of 6' in text or '6 pack' in text or '6-pack' in text:
        return 6
    if 'pack of 4' in text or '4 pack' in text or '4-pack' in text:
        return 4
    if 'pack of 3' in text or '3 pack' in text or '3-pack' in text:
        return 3
    if 'pack of 2' in text or '2 pack' in text or '2-pack' in text:
        return 2
    
    # Regex patterns
    patterns = [
        r'pack of (\d+)',
        r'(\d+)\s*pack',
        r'(\d+)\s*count',
        r'case of (\d+)',
        r'(\d+)\s*per case',
        r'set of (\d+)'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            qty = int(match.group(1))
            if 1 <= qty <= 100:  # Reasonable range
                return qty
    
    return 1

train['ipq'] = train['catalog_content'].apply(extract_ipq)
test['ipq'] = test['catalog_content'].apply(extract_ipq)

print(f"IPQ stats (train):")
print(f"  Mean: {train['ipq'].mean():.2f}")
print(f"  Median: {train['ipq'].median()}")
print(f"  Max: {train['ipq'].max()}")
print(f"\nIPQ distribution:")
print(train['ipq'].value_counts().head(10))

# Create basic features
print("\n4. Creating basic features...")
train['ipq_log'] = np.log1p(train['ipq'])
test['ipq_log'] = np.log1p(test['ipq'])

train['ipq_sqrt'] = np.sqrt(train['ipq'])
test['ipq_sqrt'] = np.sqrt(test['ipq'])

# Text length
train['text_len'] = train['catalog_content'].fillna('').str.len()
test['text_len'] = test['catalog_content'].fillna('').str.len()

train['word_count'] = train['catalog_content'].fillna('').str.split().str.len()
test['word_count'] = test['catalog_content'].fillna('').str.split().str.len()

# Simple keyword features
train['has_organic'] = train['catalog_content'].fillna('').str.lower().str.contains('organic').astype(int)
test['has_organic'] = test['catalog_content'].fillna('').str.lower().str.contains('organic').astype(int)

train['has_premium'] = train['catalog_content'].fillna('').str.lower().str.contains('premium|deluxe|gourmet').astype(int)
test['has_premium'] = test['catalog_content'].fillna('').str.lower().str.contains('premium|deluxe|gourmet').astype(int)

# Interaction features
train['ipq_x_len'] = train['ipq'] * train['text_len'] / 1000
test['ipq_x_len'] = test['ipq'] * test['text_len'] / 1000

print("  âœ… Basic features created")

# TF-IDF Features (IMPORTANT!)
print("\n5. Creating TF-IDF features...")
print("  This may take a few minutes...")

# Initialize TF-IDF
tfidf = TfidfVectorizer(
    max_features=150,      # Number of TF-IDF features
    min_df=5,              # Ignore terms that appear in less than 5 documents
    max_df=0.9,            # Ignore terms that appear in more than 90% of documents
    ngram_range=(1, 2),    # Use both unigrams and bigrams
    sublinear_tf=True      # Apply sublinear tf scaling
)

# Combine train and test text for fitting
all_text = pd.concat([
    train['catalog_content'].fillna(''),
    test['catalog_content'].fillna('')
])

print(f"  Fitting TF-IDF on {len(all_text):,} documents...")
tfidf.fit(all_text)

# Transform train and test
print("  Transforming train data...")
train_tfidf = tfidf.transform(train['catalog_content'].fillna(''))
print("  Transforming test data...")
test_tfidf = tfidf.transform(test['catalog_content'].fillna(''))

print(f"  âœ… TF-IDF features: {train_tfidf.shape[1]}")

# Combine all features
print("\n6. Combining all features...")

basic_features = ['ipq', 'ipq_log', 'ipq_sqrt', 'text_len', 'word_count', 
                  'has_organic', 'has_premium', 'ipq_x_len']

X_train = np.hstack([
    train[basic_features].values,
    train_tfidf.toarray()
])

X_test = np.hstack([
    test[basic_features].values,
    test_tfidf.toarray()
])

print(f"  Final feature shape: {X_train.shape}")
print(f"  Total features: {X_train.shape[1]} ({len(basic_features)} basic + {train_tfidf.shape[1]} TF-IDF)")

# Prepare target
y = train['price'].values

print(f"\n7. Price statistics:")
print(f"  Min: ${y.min():.2f}")
print(f"  Max: ${y.max():,.2f}")
print(f"  Mean: ${y.mean():.2f}")
print(f"  Median: ${y.median():.2f}")

# CRITICAL: Log transform
y_log = np.log1p(y)
print(f"\nLog-transformed prices:")
print(f"  Min: {y_log.min():.2f}")
print(f"  Max: {y_log.max():.2f}")

# Train model
print("\n8. Training LightGBM with 5-fold CV...")

params = {
    'objective': 'regression',
    'metric': 'mae',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': -1,
    'random_state': 42
}

# 5-fold CV
n_folds = 5
oof_preds = np.zeros(len(X_train))
test_preds = np.zeros(len(X_test))

kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)

for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train)):
    print(f"\n  Fold {fold+1}/{n_folds}")
    
    X_tr, X_val = X_train[tr_idx], X_train[val_idx]
    y_tr, y_val = y_log[tr_idx], y_log[val_idx]
    
    train_data = lgb.Dataset(X_tr, label=y_tr)
    val_data = lgb.Dataset(X_val, label=y_val)
    
    model = lgb.train(
        params,
        train_data,
        num_boost_round=1000,
        valid_sets=[val_data],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),
            lgb.log_evaluation(0)  # Silent
        ]
    )
    
    oof_preds[val_idx] = model.predict(X_val)
    test_preds += model.predict(X_test) / n_folds
    
    fold_smape = smape(np.expm1(y_val), np.expm1(oof_preds[val_idx]))
    print(f"    SMAPE: {fold_smape:.2f}%")

# Overall score
cv_smape = smape(y, np.expm1(oof_preds))

print("\n" + "="*70)
print("RESULTS")
print("="*70)
print(f"\nðŸ“Š CV SMAPE: {cv_smape:.2f}%")

if cv_smape < 15:
    print("ðŸŽ‰ EXCELLENT! Ready for competition!")
elif cv_smape < 20:
    print("âœ… GREAT! Very competitive score!")
elif cv_smape < 25:
    print("ðŸ‘ GOOD! Solid performance!")
elif cv_smape < 30:
    print("âš ï¸  FAIR - Can be improved with ensemble/images")
else:
    print("âŒ NEEDS WORK - Check diagnostics below")

# Create submission
print("\n9. Creating submission...")
final_preds = np.expm1(test_preds)
final_preds = np.maximum(final_preds, 0.01)  # Ensure positive

submission = pd.DataFrame({
    'sample_id': test['sample_id'],
    'price': final_preds
})

submission.to_csv('output/test_out_with_tfidf.csv', index=False)

print(f"\nâœ… Saved: output/test_out_with_tfidf.csv")
print(f"Predictions: {len(submission):,}")
print(f"Price range: ${submission['price'].min():.2f} - ${submission['price'].max():,.2f}")
print(f"Median: ${submission['price'].median():.2f}")

# Save OOF for analysis
oof_df = pd.DataFrame({
    'sample_id': train['sample_id'],
    'actual': y,
    'predicted': np.expm1(oof_preds),
    'error': np.abs(y - np.expm1(oof_preds)),
    'ipq': train['ipq']
})
oof_df.to_csv('output/oof_with_tfidf.csv', index=False)
print(f"âœ… OOF predictions: output/oof_with_tfidf.csv")

# Feature importance
print("\n10. Top 15 Important Features:")
if hasattr(model, 'feature_importance'):
    importance = model.feature_importance()
    feature_names = basic_features + [f'tfidf_{i}' for i in range(train_tfidf.shape[1])]
    
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    print(importance_df.head(15).to_string(index=False))

# Analysis
print("\n" + "="*70)
print("ANALYSIS")
print("="*70)

# Check worst predictions
oof_prices = np.expm1(oof_preds)
errors = np.abs(y - oof_prices)
pct_errors = errors / y * 100

print(f"\nPrediction Quality:")
print(f"  Mean Absolute Error: ${errors.mean():.2f}")
print(f"  Median Absolute Error: ${np.median(errors):.2f}")
print(f"  Mean % Error: {pct_errors.mean():.2f}%")

worst_idx = errors.argsort()[-5:]
print("\nWorst 5 predictions:")
for idx in worst_idx:
    print(f"\n  Sample {train.iloc[idx]['sample_id']}:")
    print(f"    Actual: ${y[idx]:.2f}")
    print(f"    Predicted: ${oof_prices[idx]:.2f}")
    print(f"    Error: ${errors[idx]:.2f}")
    print(f"    IPQ: {train.iloc[idx]['ipq']}")
    print(f"    Text: {str(train.iloc[idx]['catalog_content'])[:80]}...")

print("\n" + "="*70)
print("NEXT STEPS TO IMPROVE")
print("="*70)

if cv_smape < 20:
    print("\nâœ… Excellent baseline! To reach <15%:")
    print("  1. Use ensemble (LightGBM + XGBoost + CatBoost)")
    print("  2. Add image features")
    print("  3. Hyperparameter tuning")
elif cv_smape < 25:
    print("\nðŸ‘ Good baseline! To reach <20%:")
    print("  1. Increase TF-IDF features to 200-300")
    print("  2. Add more interaction features")
    print("  3. Use ensemble models")
else:
    print("\nâš ï¸  To improve:")
    print("  1. Check if IPQ extraction is working (see distribution above)")
    print("  2. Verify TF-IDF features are meaningful")
    print("  3. Review worst predictions above")

print("\n" + "="*70)
print("SUMMARY")
print("="*70)
print(f"\nFeatures used: {X_train.shape[1]}")
print(f"  - Basic features: {len(basic_features)}")
print(f"  - TF-IDF features: {train_tfidf.shape[1]}")
print(f"\nCV SMAPE: {cv_smape:.2f}%")
print(f"Expected improvement: 5-10 percentage points vs baseline")
print("\nâœ… Submission ready: output/test_out_with_tfidf.csv")
print("="*70)